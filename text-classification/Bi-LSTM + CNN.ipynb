{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bi-LSTM + CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ce9EQ2JXNgfK"},"source":["# Import Packages and Libraries"]},{"cell_type":"code","metadata":{"id":"gtJmIK1ceoZ6"},"source":["#import necessary packages and libraries\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, Add\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n","from tensorflow.keras.layers import LSTM, Bidirectional\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping\n","import gensim\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","import re\n","import codecs\n","import matplotlib.pyplot as plt\n","from gensim.models import word2vec\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zhuQ_aDNgfW"},"source":["# Read the data"]},{"cell_type":"code","metadata":{"id":"3mBxtg9Hk7eY"},"source":["#Read the data and import them into dataframes\n","df1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fake.csv')\n","df2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/True.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBd1kupXNgfb"},"source":["# Data Cleaning and Preprocessing"]},{"cell_type":"code","metadata":{"id":"Dhb0szbok2Cp"},"source":["#Assign target variable to differentiate fake and real news\n","df1['target'] = [1 for i in range(len(df1))]\n","df2['target'] = [0 for i in range(len(df2))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAnFbO5voVlX"},"source":["#Concatenate real and fake news datsets and clean the data\n","df_tot = pd.concat([df1, df2])\n","df_tot = df_tot.replace(r'^\\s*$', np.NaN, regex=True)\n","df_tot = df_tot.dropna()\n","df_tot = df_tot.reset_index()\n","df_tot = df_tot.drop('index', axis=1)\n","df_tot['combined'] = df_tot['title'] + '. ' + df_tot['text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBc1tjhPoWOJ"},"source":["#Clean the datsets by removing special characters, numbers etc.\n","def news_wordlist(new, remove_stopwords=False):\n","    new = re.sub(\"[^a-zA-Z]\",\" \", new)\n","    words = new.lower().split()\n","    if remove_stopwords:\n","        stops = set(stopwords.words(\"english\"))     \n","        words = [w for w in words if not w in stops] \n","    return (words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4AUCvZworks","outputId":"db140546-2090-4fea-8ad4-a47bbce16bd3","colab":{"base_uri":"https://localhost:8080/"}},"source":["#define function to return news after cleaning\n","def news_sentences(new, remove_stopwords=False):\n","    raw_sentences = nltk.sent_tokenize(new.strip())\n","    sentences = []\n","    # 2. Loop for each sentence\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 0:\n","            sentences.append(news_wordlist(raw_sentence, remove_stopwords))\n","\n","    # This returns the list of lists\n","    return sentences\n","\n","sentences = []\n","\n","for new in df_tot[\"combined\"]:\n","    sentences += news_sentences(new)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"38giVDQfNgfs"},"source":["#Add each word in the combined column of news datset into vocab list\n","vocab = Counter()\n","\n","for new in df_tot[\"combined\"]:\n","    raw_sentences = nltk.sent_tokenize(new.strip())\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 0:\n","            vocab.update(news_wordlist(raw_sentence, remove_stopwords=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9hea7ttNgfy"},"source":["# Create Word2Vec"]},{"cell_type":"code","metadata":{"id":"fRH31iY2o8R4","outputId":"460ccdd8-a72c-4c91-cf30-80f5d41ce770","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Create Word2Vec model and store it in model\n","#Word2Vec\n","num_features = 100  \n","min_word_count = 5\n","num_workers = 4     \n","context = 10        \n","downsampling = 1e-3 \n","\n","# Initializing the train model\n","print(\"Training model....\")\n","model = word2vec.Word2Vec(sentences,\\\n","                          workers=num_workers,\\\n","                          size=num_features,\\\n","                          min_count=min_word_count,\\\n","                          window=context,\n","                          sample=downsampling)\n","\n","# To make the model memory efficient\n","model.init_sims(replace=True)\n","\n","# Saving the model for later use. Can be loaded using Word2Vec.load()\n","model_name = \"word2vec_model\"\n","model.save(model_name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training model....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xwlH_LzynD-0"},"source":["#Assign X and Y variables where y is target variable\n","X = df_tot[['combined']]\n","y = df_tot['target']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DFnc6DUnF3M"},"source":["#Split data into training data and testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Plc7AeoqnIV5"},"source":["#Put the X_Train and X_Test into a list called X_train_sent and X_test_sent\n","X_train_sent = list(X_train[\"combined\"].values)\n","X_test_sent = list(X_test[\"combined\"].values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-VSTTQTpnT8E"},"source":["# CNN Model"]},{"cell_type":"code","metadata":{"id":"eK1y2vVYnQdM"},"source":["#Import necessary libraries for CNN\n","from collections import Counter\n","from keras.preprocessing.text import Tokenizer\n","import itertools\n","from keras.preprocessing.sequence import pad_sequences\n","#from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\n","from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n","from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model, load_model\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras import backend as K\n","from keras.engine import InputSpec, Layer\n","from keras.optimizers import Adam\n","\n","\n","from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n","from tensorflow.python.keras.layers import LSTM, CuDNNLSTM\n","from tensorflow.python.keras.layers import GRU, CuDNNGRU\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_sjQGHVMF4Cm"},"source":["word_vectors = model.wv\n","MAX_NB_WORDS = len(word_vectors.vocab)\n","MAX_SEQUENCE_LENGTH = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIAuTYVQF4qU"},"source":["#preprocess and clean train and test data \n","X_train_sent_pre = []\n","for new in X_train_sent:\n","    news_sent = news_sentences(new)\n","    news_sent = list(itertools.chain(*news_sent))\n","    X_train_sent_pre.append(news_sent)\n","\n","X_test_sent_pre = []\n","for new in X_test_sent:\n","    news_sent = news_sentences(new)\n","    news_sent = list(itertools.chain(*news_sent))\n","    X_test_sent_pre.append(news_sent)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCYFQXlWyDil"},"source":["#assign values for the words in each sentence in the training and test dataset using word2vec model\n","word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n","sequences = [[word_index.get(t, 0) for t in sentence]\n","             for sentence in X_train_sent_pre]\n","test_sequences = [[word_index.get(t, 0)  for t in sentence] \n","                  for sentence in X_test_sent_pre]\n","\n","# pad the sequences\n","X_train_seq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n","                     padding=\"pre\", truncating=\"post\")\n","X_test_seq = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n","                          truncating=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdtTs-W8_DZl","outputId":"c803abfd-f32b-43c5-eb04-48ed2f6e67b2","colab":{"base_uri":"https://localhost:8080/"}},"source":["#shape of X_test_seq\n","X_test_seq.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(22134, 200)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"0ezp6U1aGCmx"},"source":["WV_DIM = 100\n","nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab)) +1\n","# we initialize the matrix with random numbers\n","wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n","for word, i in word_index.items():\n","    if i >= MAX_NB_WORDS:\n","        continue\n","    try:\n","        embedding_vector = word_vectors[word]\n","        # words not found in embedding index will be all-zeros.\n","        wv_matrix[i] = embedding_vector\n","    except:\n","        pass  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dglmDNYiF5HR"},"source":["#Embedding layer\n","wv_layer = Embedding(nb_words,\n","                     WV_DIM,\n","                     mask_zero=False,\n","                     weights=[wv_matrix],\n","                     input_length=MAX_SEQUENCE_LENGTH,\n","                     trainable=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzsdsH3Oo4Z1"},"source":["#define attention layer\n","class Attention(tf.keras.Model):\n","    def __init__(self, units):\n","        super(Attention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, features, hidden):\n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","          \n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","        score = tf.nn.tanh(\n","            self.W1(features) + self.W2(hidden_with_time_axis))\n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","          \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * features\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzzL6IvjHhma"},"source":["#Define function to build the model\n","def build_model(wv_layer):\n","    input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","    x = wv_layer(input)\n","    conv1 = Conv1D(filters=1, kernel_size=2, padding='same')(x)\n","    conv1 = MaxPooling1D(pool_size=32)(conv1)  \n","    conv2 = Conv1D(filters=2, kernel_size=3, padding='same')(x)\n","    conv2 = MaxPooling1D(pool_size=32)(conv2)\n","        \n","    conv3 = Conv1D(filters=3, kernel_size=4, padding='same')(x)\n","    conv3 = MaxPooling1D(pool_size=32)(conv3)\n","        \n","    cnn = concatenate([conv1, conv2, conv3], axis=-1)\n","    # flat = Flatten()(cnn)\n","    x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.25,\n","                           recurrent_dropout=0.25))(cnn)\n","    (lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(300, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(x)\n","    state_h = concatenate([forward_h, backward_h])\n","    state_c = concatenate([forward_c, backward_c])\n","    context_vector, attention_weights = Attention(10)(x, state_h)\n","    x = Dense(256, activation=\"relu\")(context_vector)\n","    x = Dropout(0.25)(x)\n","    x = Dense(1, activation=\"sigmoid\")(x)\n","    model = Model(inputs=input, outputs=x)\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9lOTPfnzBYy","outputId":"f0050dce-bf84-452d-9535-abdc5b52c7e5","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Build the model\n","model = build_model(wv_layer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 200)]        0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 200, 100)     4497600     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 200, 1)       201         embedding[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 200, 2)       602         embedding[0][0]                  \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 200, 3)       1203        embedding[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d (MaxPooling1D)    (None, 6, 1)         0           conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","max_pooling1d_1 (MaxPooling1D)  (None, 6, 2)         0           conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_2 (MaxPooling1D)  (None, 6, 3)         0           conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 6, 6)         0           max_pooling1d[0][0]              \n","                                                                 max_pooling1d_1[0][0]            \n","                                                                 max_pooling1d_2[0][0]            \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 6, 600)       736800      concatenate[0][0]                \n","__________________________________________________________________________________________________\n","bi_lstm_1 (Bidirectional)       [(None, 6, 600), (No 2162400     bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 600)          0           bi_lstm_1[0][1]                  \n","                                                                 bi_lstm_1[0][3]                  \n","__________________________________________________________________________________________________\n","attention (Attention)           ((None, 600), (None, 12031       bidirectional[0][0]              \n","                                                                 concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 256)          153856      attention[0][0]                  \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 256)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            257         dropout[0][0]                    \n","==================================================================================================\n","Total params: 7,564,950\n","Trainable params: 3,067,350\n","Non-trainable params: 4,497,600\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"trmHva7dsxAe"},"source":["epoch = 10"]},{"cell_type":"code","metadata":{"id":"of9RWKZmsvXo","outputId":"bc706587-ce4b-432f-bde7-e2d88b5917e8","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Running model when epoch=10 and printing results\n","history = model.fit(X_train_seq, y_train, validation_split=0.1,\n","                 epochs=10, batch_size=256, shuffle=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","78/78 [==============================] - 64s 820ms/step - loss: 0.1600 - accuracy: 0.9408 - val_loss: 0.0781 - val_accuracy: 0.9715\n","Epoch 2/10\n","78/78 [==============================] - 63s 803ms/step - loss: 0.0864 - accuracy: 0.9688 - val_loss: 0.0396 - val_accuracy: 0.9860\n","Epoch 3/10\n","78/78 [==============================] - 65s 830ms/step - loss: 0.0530 - accuracy: 0.9810 - val_loss: 0.0170 - val_accuracy: 0.9946\n","Epoch 4/10\n","78/78 [==============================] - 63s 807ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 0.0077 - val_accuracy: 0.9977\n","Epoch 5/10\n","78/78 [==============================] - 65s 838ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.0059 - val_accuracy: 0.9986\n","Epoch 6/10\n","78/78 [==============================] - 64s 819ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0050 - val_accuracy: 0.9991\n","Epoch 7/10\n","78/78 [==============================] - 64s 818ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0053 - val_accuracy: 0.9991\n","Epoch 8/10\n","78/78 [==============================] - 64s 815ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0050 - val_accuracy: 0.9986\n","Epoch 9/10\n","78/78 [==============================] - 64s 822ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.0067 - val_accuracy: 0.9982\n","Epoch 10/10\n","78/78 [==============================] - 63s 811ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0043 - val_accuracy: 0.9991\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e6otSbw_s1LL","outputId":"e0c08ae8-8f35-436d-ceba-495429acf24e","colab":{"base_uri":"https://localhost:8080/"}},"source":["test_loss, test_acc = model.evaluate(X_test_seq, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["692/692 [==============================] - 32s 47ms/step - loss: 0.0071 - accuracy: 0.9985\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-obemtR0s3AR","outputId":"663fae31-a9de-4e32-b684-3f97d055adb8","colab":{"base_uri":"https://localhost:8080/"}},"source":["#print accuracy when epoch=10\n","test_acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9984639286994934"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"DL9Noy05Ngg1"},"source":["# Print Model Results"]},{"cell_type":"code","metadata":{"id":"C-AC20v4naCc"},"source":["#print overall reults of the model when epoch=10\n","from sklearn.metrics import classification_report\n","\n","y_pred = model.predict(X_test_seq, batch_size=64, verbose=1)\n","y_pred_bool = (y_pred > 0.5).astype(\"int32\")\n","\n","print(classification_report(y_test, y_pred_bool))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sa-XxtPgo80U"},"source":["#adding binary prediction(0/1) into list called binary_prediction\n","pred = model.predict(X_test_seq)\n","\n","binary_predictions = []\n","\n","for i in pred:\n","    if i >= 0.5:\n","        binary_predictions.append(1)\n","    else:\n","        binary_predictions.append(0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T53ni8t8pgRf","outputId":"06c12c6a-0ced-4063-f23f-7bb129cb6d33","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Printing results score of model when epoch=10\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n","accuracy = accuracy_score(binary_predictions, y_test)\n","precision = precision_score(binary_predictions, y_test)\n","recall = recall_score(binary_predictions, y_test)\n","print('Accuracy on testing set:', accuracy)\n","print('Precision on testing set:', precision)\n","print('Recall on testing set:', recall)\n","f1 = 2 * (precision * recall) / (precision + recall)\n","print('Recall on testing set:',f1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy on testing set: 0.997334417638023\n","Precision on testing set: 0.9959503477418787\n","Recall on testing set: 0.9988521984813703\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0KJK0DiIwYs-"},"source":["cm = pd.DataFrame(confusion_matrix(y_test,y_pred_bool) , index = ['Fake','Not Fake'] , columns = ['Fake','Not Fake'])\n","sns.heatmap(cm,cmap= 'Blues', annot = True, fmt='', xticklabels = ['Fake','Not Fake'], yticklabels = ['Fake','Not Fake'])\n","plt.xlabel(\"Actual\")\n","plt.ylabel(\"Predicted\")\n","plt.title('Confusion matrix on test data')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGyb48VNNghE"},"source":["# Function to test on external datasets"]},{"cell_type":"code","metadata":{"id":"qDoJ552J3u2v"},"source":["#Define function for testing on other datasets\n","def test1(df, vocab):\n","    X_test_sent = list(df[\"text\"].values)\n","    X_test_sent_pre = []\n","    #vocab = Counter()\n","    for new in df['text']:\n","        raw_sentences = nltk.sent_tokenize(new.strip())\n","        for raw_sentence in raw_sentences:\n","            if len(raw_sentence) > 0:\n","                vocab.update(news_wordlist(raw_sentence, remove_stopwords=False))\n","    for new in X_test_sent:\n","        news_sent = news_sentences(new)\n","        news_sent = list(itertools.chain(*news_sent))\n","        X_test_sent_pre.append(news_sent)\n","    word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n","    test_sequences = [[word_index.get(t, 0)  for t in sentence] \n","                    for sentence in X_test_sent_pre]\n","    X_train_seq = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, \n","                     padding=\"pre\", truncating=\"post\")\n","    result = model.predict(X_train_seq)\n","    result1 = np.argmax(result, axis=1)\n","    return result1"],"execution_count":null,"outputs":[]}]}